
#' Preprocess a single subject
#' @param scfg A list of configuration settings
#' @param sub_cfg A data.frame of subject configuration settings
#' @param steps A named logical vector indicating which steps to run
#' @return A logical value indicating whether the preprocessing was successful
#' @importFrom glue glue
#' @importFrom checkmate assert_class assert_list assert_names assert_logical
#' @keywords internal
process_subject <- function(scfg, sub_cfg = NULL, steps = NULL) {
  checkmate::assert_class(scfg, "bg_study_cfg")
  checkmate::assert_data_frame(sub_cfg)
  expected_fields <- c("sub_id", "ses_id", "dicom_sub_dir", "dicom_ses_dir", "bids_sub_dir", "bids_ses_dir")
  checkmate::assert_names(names(sub_cfg), must.include = expected_fields, type = "unique")
  stopifnot(length(unique(sub_cfg$sub_id)) == 1L)
  multi_session <- nrow(sub_cfg) > 1L
  if (multi_session) {
    if (any(is.na(sub_cfg$ses_id))) stop("Session IDs are required for multi-session inputs to process_subject.")
    if (any(duplicated(sub_cfg$ses_id))) stop("Duplicate session IDs found in sub_cfg. process_subject requires unique session IDs.")
  }
  checkmate::assert_logical(steps, names = "unique")
  expected <- c("bids_conversion", "bids_validation", "mriqc", "fmriprep", "aroma", "postprocess")
  for (ee in expected) if (is.na(steps[ee])) steps[ee] <- FALSE # ensure we have valid logicals for expected fields

  sub_id <- sub_cfg$sub_id[1L]
  bids_sub_dir <- sub_cfg$bids_sub_dir[1L]
  lg <- get_subject_logger(scfg, sub_id)
  
  bids_conversion_ids <- bids_validation_id <- mriqc_id <- fmriprep_id <- aroma_id <- postprocess_ids <- NULL

  # N.B. fmriprep processes a subject, not a session... Thus, we need to submit a top-level job for the subject

  # BIDS conversion and postprocessing are session-specific, so we need to check for the session ID
  # BIDS validation, fmriprep, mriqc, and aroma are subject-specific (sessions nested within subjects)

  # .*complete files should always be placed in the subject BIDS directory
  # determine status of processing -- seems like we could swap in queries from job tracker
  submit_step <- function(name, row_idx = 1L, parent_ids = NULL) {
    session_level <- name %in% c("bids_conversion", "postprocess") # only these two are session-level

    sub_id <- sub_cfg$sub_id[row_idx]
    ses_id <- sub_cfg$ses_id[row_idx]
    has_ses <- !is.na(ses_id)
    ses_str <- ifelse(has_ses && session_level, glue("_ses-{ses_id}"), "") # qualifier for .complete file
    complete_file <- file.path(sub_cfg$bids_sub_dir[row_idx], glue(".{name}{ses_str}_complete"))
    file_exists <- checkmate::test_file_exists(complete_file)

    job_id <- NULL
    # skip out if this step is not requested or it is already complete
    if (!steps[name] || (file_exists && !scfg$force)) {
      if (file_exists) {
        lg$debug("Skipping {name} for {sub_id} because .{name}{ses_str}_complete file already exists.")
      } else {
        lg$debug("Skipping {name} for {sub_id} because step is not requested.")
      }
      return(job_id)
    }

    # clear existing complete file if we are starting over on this step
    if (file_exists) {
      lg$debug("Removing existing .{name}{ses_str}_complete file {complete_file}")
      unlink(complete_file)
    }

    # shared components across specific jobs
    jobid_str <- ifelse(has_ses, glue("{name}-sub-{sub_id}_ses-{ses_id}"), glue("{name}-sub-{sub_id}"))
    env_variables <- c(
      debug_pipeline = scfg$debug,
      pkg_dir = system.file(package = "BGprocess"), # root of inst folder for installed R package
      cmd_log = lg$appenders$subject_logger$destination, # write to same file as subject lgr
      stdout_log = glue("{scfg$log_directory}/sub-{sub_id}/{jobid_str}-%j-{format(Sys.time(), '%d%b%Y_%H.%M.%S')}.out"),
      stderr_log = glue("{scfg$log_directory}/sub-{sub_id}/{jobid_str}-%j-{format(Sys.time(), '%d%b%Y_%H.%M.%S')}.err")
    )
    sched_script <- get_job_script(scfg, name)
    sched_args <- get_job_sched_args(scfg, name)
    sched_args <- set_cli_options( # setup files for stdout and stderr, job name
      sched_args,
      c(
        glue("--job-name={jobid_str}"),
        glue("--output={env_variables['stdout_log']}"),
        glue("--error={env_variables['stderr_log']}")
      )
    )

    # determine the directory to use for the job submission
    if (session_level && has_ses) {
      # if it's a session-level process and we have a valid session-level input, use the session directory
      dir <- ifelse(name == "bids_conversion", sub_cfg$dicom_ses_dir[row_idx], sub_cfg$bids_ses_dir[row_idx])
    } else {
      # if it's a subject-level process or we don't have a valid session-level input, use the subject directory
      dir <- ifelse(name == "bids_conversion", sub_cfg$dicom_sub_dir[row_idx], sub_cfg$bids_sub_dir[row_idx])
    }

    # launch submission function -- these all follow the same input argument structure
    lg$debug("Launching submit_{name} for subject: {sub_id}")
    job_id <- do.call(glue("submit_{name}"), list(scfg, dir, sub_id, ses_id, env_variables, sched_script, sched_args, parent_ids, lg))

    return(job_id)
  }

  ## Ensure that log directory exists
  dir.create(file.path(scfg$log_directory, paste("sub", sub_id, sep = "-")), showWarnings = FALSE, recursive = TRUE)
  lg$info(glue("Processing subject {sub_id} with {nrow(sub_cfg)} sessions."))
  # lg$info(glue("Processing steps: {glue_collapse(names(steps), sep = ', ')}"))
  lg$info(glue("BIDS directory: {bids_sub_dir}"))

  ## Handle BIDS conversion -- session-level
  n_inputs <- nrow(sub_cfg)

  # need unlist because NULL will be returned for jobs not submitted -- yielding a weird list of NULLs
  bids_conversion_ids <- unlist(lapply(seq_len(n_inputs), function(idx) submit_step("bids_conversion", row_idx = idx)))
  
  if (isTRUE(steps["bids_conversion"])) {  
    # Use expected directory as input to subsequent steps, anticipating that conversion completes
    # and the expected directory is created. If conversion fails, the dependent jobs should automatically fail.
    bids_sub_dir <- file.path(scfg$bids_directory, glue("sub-{sub_cfg$sub_id[1L]}"))
    bids_ses_dir <- if (multi_session) file.path(scfg$bids_directory, glue("sub-{sub_cfg$sub_id}"), glue("ses-{sub_cfg$ses_id}")) else rep(NA_character_, nrow(sub_cfg))

    # When bids_sub_dir and bids_ses_dir exist, do they match these expectations?
    extant_bids <- !is.na(sub_cfg$bids_sub_dir)
    if (!identical(sub_cfg$bids_sub_dir[extant_bids], bids_sub_dir[extant_bids])) {
      lg$warn(glue("Exiting process_subject for {sub_id} because expected BIDS directory does not match: {bids_sub_dir}"))
      return(TRUE)
    }

    extant_bids_ses <- !is.na(sub_cfg$bids_ses_dir)
    if (multi_session && !identical(sub_cfg$bids_ses_dir[extant_bids_ses], bids_ses_dir[extant_bids_ses])) {
      lg$warn(glue("Exiting process_subject for {sub_id} because expected BIDS session directory does not match: {bids_ses_dir[1L]}"))
      return(TRUE)
    }
    
  } else if (!checkmate::test_directory_exists(bids_sub_dir)) {
    lg$warn(glue("Exiting process_subject for {sub_id} because expected BIDS directory does not exist: {bids_sub_dir}"))
    return(TRUE)
  }

  # Everything after BIDS conversion depends on the BIDS directory existing

  ## Handle BIDS validation
  # on further investigation, bids-validator only works on the root of the BIDS directory.
  # so, maybe we should just let fmriprep handle this directly
  # bids_validation_id <- submit_step("bids_validation", parent_ids = bids_conversion_ids)

  ## Handle MRIQC
  mriqc_id <- submit_step("mriqc", parent_ids = bids_conversion_ids)

  ## Handle fmriprep
  fmriprep_id <- submit_step("fmriprep", parent_ids = bids_conversion_ids)

  ## Handle aroma
  aroma_id <- submit_step("aroma", parent_ids = c(bids_conversion_ids, fmriprep_id))

  ## Handle postprocessing
  postprocess_ids <- unlist(lapply(seq_len(n_inputs), function(idx) submit_step("postprocess", row_idx = idx, parent_ids = c(bids_conversion_ids, bids_validation_id, fmriprep_id, aroma_id))))

  return(TRUE) # nothing interesting for now
}


submit_bids_conversion <- function(scfg, sub_dir = NULL, sub_id = NULL, ses_id = NULL, env_variables = NULL, sched_script = NULL, sched_args = NULL, parent_ids = NULL, lg = NULL) {
  # heudiconv  --files dicom/219/itbs/*/*.dcm -o Nifti -f Nifti/code/heuristic1.py -s 219 -ss itbs -c dcm2niix -b --minmeta --overwrite

  env_variables <- c(
    env_variables,
    heudiconv_container = scfg$compute_environment$heudiconv_container,
    loc_sub_dicoms = sub_dir,
    loc_bids_root = scfg$bids_directory,
    heudiconv_heuristic = scfg$heudiconv$heuristic_file,
    validate_bids = scfg$heudiconv$validate_bids,
    sub_id = sub_id,
    ses_id = ses_id
  )

  job_id <- fmri.pipeline::cluster_job_submit(sched_script,
    scheduler = scfg$compute_environment$scheduler,
    sched_args = sched_args, env_variables = env_variables,
    wait_jobs = parent_ids, echo = FALSE
  )

  # log submission command
  

  return(job_id)

}

submit_bids_validation <- function(scfg, sub_dir = NULL, sub_id = NULL, ses_id = NULL, env_variables = NULL, sched_script = NULL, sched_args = NULL, parent_ids = NULL, lg = NULL) {
  
  env_variables <- c(
    env_variables,
    bids_validator = scfg$compute_environment$bids_validator,
    bids_dir = sub_dir,
    sub_id = sub_id,
    outfile = scfg$bids_validation$outfile
  )

  job_id <- fmri.pipeline::cluster_job_submit(sched_script,
    scheduler = scfg$compute_environment$scheduler,
    sched_args = sched_args, env_variables = env_variables,
    wait_jobs = parent_ids
  )

  return(job_id)
}

submit_fmriprep <- function(scfg, sub_dir = NULL, sub_id = NULL, ses_id = NULL, env_variables = NULL, sched_script = NULL, sched_args = NULL, parent_ids = NULL, lg = NULL) {
  checkmate::assert_list(scfg)
  checkmate::assert_character(parent_ids, null.ok = TRUE)

  if (!validate_exists(scfg$compute_environment$fmriprep_container)) {
    #lg$debug("Unable to submit fmriprep for {sub_dir} because $compute_environment$fmriprep_container is missing.")
    warning("Unable to submit fmriprep for {sub_dir} because $compute_environment$fmriprep_container is missing.")
    return(NULL)
  }

  if (isTRUE(scfg$run_aroma) && (is.null(scfg$fmriprep$output_spaces) || !grepl("MNI152NLin6Asym:res-2", scfg$fmriprep$output_spaces, fixed = TRUE))) {
    message("Adding MNI152NLin6Asym:res-2 to output spaces for fmriprep to allow AROMA to run.")
    scfg$fmriprep$output_spaces <- paste(scfg$fmriprep$output_spaces, "MNI152NLin6Asym:res-2")
  }

  cli_options <- set_cli_options(scfg$fmriprep$cli_options, c(
    glue("--nthreads {scfg$fmriprep$ncores}"),
    glue("--omp-nthreads {scfg$fmriprep$ncores}"),
    glue("--participant_label {sub_id}"),
    glue("-w {scfg$scratch_directory}"),
    glue("--fs-license-file {scfg$fmriprep$fs_license_file}"),
    glue("--output-spaces {scfg$fmriprep$output_spaces}"),
    glue("--mem {scfg$fmriprep$memgb*1000}") # convert to MB
  ), collapse=TRUE)

  env_variables <- c(
    env_variables,
    fmriprep_container = scfg$compute_environment$fmriprep_container,
    sub_id = sub_id,
    ses_id = ses_id,
    loc_bids_root = scfg$bids_directory,
    loc_mrproc_root = scfg$fmriprep_directory,
    loc_scratch = scfg$scratch_directory,
    templateflow_home = scfg$templateflow_home,
    fs_license_file = scfg$fmriprep$fs_license_file,
    cli_options = cli_options
  )

  job_id <- fmri.pipeline::cluster_job_submit(sched_script,
    scheduler = scfg$compute_environment$scheduler,
    sched_args = sched_args, env_variables = env_variables,
    wait_jobs = parent_ids
  )

  return(job_id)
}


submit_mriqc <- function(scfg, sub_dir = NULL, sub_id = NULL, ses_id = NULL, env_variables = NULL, sched_script = NULL, sched_args = NULL, parent_ids = NULL, lg = NULL) {
   if (!validate_exists(scfg$compute_environment$mriqc_container)) {
    message(glue("Skipping MRIQC in {sub_dir} because could not find MRIQC container {scfg$compute_environment$mriqc_container}"))
    return(NULL)
  }

  cli_options <- set_cli_options(scfg$mriqc$cli_options, c(
    glue("--nprocs {scfg$mriqc$ncores}"),
    glue("--omp-nthreads {scfg$mriqc$ncores}"),
    glue("--participant_label {sub_id}"),
    glue("-w {scfg$scratch_directory}"),
    glue("--mem-gb {scfg$mriqc$memgb}")
  ), collapse=TRUE)

  env_variables <- c(
    env_variables,
    mriqc_container = scfg$compute_environment$mriqc_container,
    sub_id = sub_id,
    ses_id = ses_id,
    loc_bids_root = scfg$bids_directory,
    loc_mriqc_root = scfg$mriqc_directory,
    loc_scratch = scfg$scratch_directory,
    cli_options = cli_options
  )

  job_id <- fmri.pipeline::cluster_job_submit(sched_script,
    scheduler = scfg$compute_environment$scheduler,
    sched_args = sched_args, env_variables = env_variables,
    wait_jobs = parent_ids
  )

  return(job_id)
}

submit_aroma <- function(scfg, sub_dir = NULL, sub_id = NULL, ses_id = NULL, env_variables = NULL, sched_script = NULL, sched_args = NULL, parent_ids = NULL, lg = NULL) {
   if (!validate_exists(scfg$compute_environment$aroma_container)) {
    message(glue("Skipping AROMA in {sub_dir} because could not find AROMA container {scfg$compute_environment$aroma_container}"))
    return(NULL)
  }

  # TODO: unclear whether we need run_aroma and "aroma" in steps...
  if (!isTRUE(scfg$run_aroma)) {
    message(glue("Skipping AROMA in {sub_dir} because run_aroma is FALSE"))
    return(NULL)
  }

  # for now, inherit key options from fmriprep rather than asking user to respecify
  # https://fmripost-aroma.readthedocs.io/latest/usage.html

  cli_options <- set_cli_options(scfg$aroma$cli_options, c(
    glue("--nthreads {scfg$aroma$ncores}"),
    glue("--omp-nthreads {scfg$aroma$ncores}"),
    glue("--participant_label {sub_id}"),
    glue("-w {scfg$scratch_directory}"),
    glue("--mem {scfg$aroma$memgb*1000}"), # convert to MB
    glue("--derivatives fmriprep={scfg$fmriprep_directory}")
  ), collapse=TRUE)

  env_variables <- c(
    env_variables,
    aroma_container = scfg$compute_environment$aroma_container,
    sub_id = sub_id,
    ses_id = ses_id,
    loc_bids_root = scfg$bids_directory,
    loc_mrproc_root = scfg$fmriprep_directory,
    loc_scratch = scfg$scratch_directory,
    cli_options = cli_options
  )

  job_id <- fmri.pipeline::cluster_job_submit(sched_script,
    scheduler = scfg$compute_environment$scheduler,
    sched_args = sched_args, env_variables = env_variables,
    wait_jobs = parent_ids
  )

  return(job_id)
}


submit_postprocess <- function(scfg, sub_dir = NULL, sub_id = NULL, ses_id = NULL, env_variables = NULL, sched_script = NULL, sched_args = NULL, parent_ids = NULL, lg = NULL) {

  # postprocessing
  script <- get_job_script(scfg, "postprocess")
  sched_args <- get_job_sched_args(scfg, "postprocess")
  env_variables <- c(
    env_variables,
    loc_sub_dicoms = scfg$dicom_directory,
    loc_bids_root = scfg$bids_directory,
    heudiconv_heuristic = scfg$heudiconv$heuristic_file,
    sub = get_sub_id(sub_dir)
  )

  job_id <- fmri.pipeline::cluster_job_submit(script,
    scheduler = scfg$compute_environment$scheduler,
    sched_args = sched_args, env_variables = env_variables,
    wait_jobs = parent_ids
  )

}
